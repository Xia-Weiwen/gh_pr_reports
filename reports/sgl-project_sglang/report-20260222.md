# AI Analysis of Pull Requests for sgl-project/sglang during 20260215-20260222

This week’s primary trend is a massive shift toward "Kernel Slimming," migrating legacy AOT CUDA kernels to a unified JIT compilation framework for improved deployment agility, alongside Day-0 support for next-generation architectures like Qwen3.5 and GLM-5 across 250 PRs in (1) JIT kernel migration, (2) Blackwell (SM100/SM120) performance tuning, (3) Diffusion framework optimizations, and (4) Distributed Disaggregation (PD) robustness.

### 1. Kernel Infrastructure & Slimming
*   **AOT to JIT Migration:** Extensive porting of kernels from `sgl-kernel` to the `jit_kernel` framework using FlashInfer JIT and TVM-FFI to reduce build complexity.
    *   Migrated `fused_add_rmsnorm` ([PR#19116](https://github.com/sgl-project/sglang/pull/19116)), `moe_fused_gate` ([PR#19039](https://github.com/sgl-project/sglang/pull/19039)), `downcast_fp8` ([PR#19103](https://github.com/sgl-project/sglang/pull/19103)), and `segment_packbits` ([PR#19088](https://github.com/sgl-project/sglang/pull/19088)).
    *   Performance gains: FlashInfer JIT implementations demonstrate 15-28% speedups over AOT for small/medium compute-bound shapes ([PR#19111](https://github.com/sgl-project/sglang/pull/19111)).
*   **MoE Kernel Enhancements:** Support for `prepare_moe_input` in JIT ([PR#19058](https://github.com/sgl-project/sglang/pull/19058)) and optimized `kimi_k2_moe_fused_gate` for 384 experts ([PR#19049](https://github.com/sgl-project/sglang/pull/19049)).
*   **Deterministic Sampling:** Refactored `sampler.py` to use MurmurHash3 for Gumbel-trick sampling, improving numerical stability for RL on-policy tasks ([PR#18915](https://github.com/sgl-project/sglang/pull/18915)).

### 2. Model Adaptation & Quantization
*   **Qwen3.5 Series:** Enabled support for NVFP4 and block-wise FP8 quantization ([PR#18965](https://github.com/sgl-project/sglang/pull/18965), [PR#18926](https://github.com/sgl-project/sglang/pull/18926)). Fixed precision bugs in MLP layers for `TP_SIZE > 1` ([PR#19070](https://github.com/sgl-project/sglang/pull/19070)).
*   **GLM-5 Implementation:** Integrated DeepSeek-style Sparse Attention (NSA) for GLM-5 and addressed precision loss in `weights_proj` by switching to FP32 GEMM outputs ([PR#19041](https://github.com/sgl-project/sglang/pull/19041)).
*   **DeepSeek V3.2:** Enhanced MLA (Multi-head Latent Attention) stability with deterministic rank ownership in mixed TP/CP disaggregated configurations ([PR#19119](https://github.com/sgl-project/sglang/pull/19119)).
*   **CompressedTensors:** Added serving support for GLM4 MoE Lite using `CompressedTensorsWNA16MarlinMoEMethod` ([PR#19106](https://github.com/sgl-project/sglang/pull/19106)).

### 3. Performance & Architecture
*   **VLM ViT Backend:** Introduced `flashinfer_cudnn` for multimodal prefill, delivering an 11.6% TTFT reduction for Qwen3-VL compared to FlashAttention-3 ([PR#19003](https://github.com/sgl-project/sglang/pull/19003)).
*   **Breakable CUDA Graphs:** Implemented `SGLANG_USE_BREAKABLE_CUDA_GRAPH` to allow selective graph breaks for incompatible ops while maintaining capture for the remainder of the forward pass ([PR#19102](https://github.com/sgl-project/sglang/pull/19102)).
*   **Input Buffer Pooling:** Refactored `GraphInputBuffers` into a pooled singleton pattern, allowing decode, prefill, and EAGLE runners to reuse the largest existing allocations for named buffers like `input_ids` ([PR#18991](https://github.com/sgl-project/sglang/pull/18991), [PR#18960](https://github.com/sgl-project/sglang/pull/18960)).
*   **Symmetric Memory:** Stabilized Piecewise CUDA Graph (PCG) with `--enable-symm-mem` by skipping Dynamo tracing in NCCL paths ([PR#19080](https://github.com/sgl-project/sglang/pull/19080)).

### 4. Diffusion & Multimodal Generation
*   **Initialization Speed:** Parallelized pipeline module loading using `ThreadPoolExecutor`, achieving ~54% faster cold-start times ([PR#19114](https://github.com/sgl-project/sglang/pull/19114)).
*   **Advanced Quantization:** Support for Nunchaku (W4A4) for Flux.1-dev and Z-Image-Turbo, yielding up to 7.5x speedups ([PR#18959](https://github.com/sgl-project/sglang/pull/18959)).
*   **USP Layer Support:** Enabled Ulysses Attention Alignment (UAA) for USP-based video generation models to handle non-divisible head counts ([PR#18990](https://github.com/sgl-project/sglang/pull/18990)).
*   **Ernie4.5-VL Optimization:** Introduced a fused Triton kernel for 3D MRoPE, reducing rotary embedding time from 670µs to 123µs ([PR#18856](https://github.com/sgl-project/sglang/pull/18856)).

### 5. Hardware Backends (AMD & NPU)
*   **AMD ROCm:** Enabled the JIT KV-cache path for ROCm/HIP ([PR#18992](https://github.com/sgl-project/sglang/pull/18992)) and added QuickReduce support via `amdsmi` integration in Docker images ([PR#19091](https://github.com/sgl-project/sglang/pull/19091)).
*   **Ascend NPU:** Added w4a4 MoE support in `modelslim` and implemented processor/embedding input handling for InternVL ([PR#18924](https://github.com/sgl-project/sglang/pull/18924), [PR#19127](https://github.com/sgl-project/sglang/pull/19127)).

### 6. Core Infrastructure & Reliability
*   **Security:** Added native SSL/TLS support for both HTTP and gRPC endpoints ([PR#18973](https://github.com/sgl-project/sglang/pull/18973)).
*   **HiRadixCache:** Added TTL-based prefix pinning with refresh-on-hit, enabling pinned blocks to survive LRU eviction to host memory until expiry ([PR#18941](https://github.com/sgl-project/sglang/pull/18941)).
*   **Dumper Tooling:** Major enhancements to the tensor debugger, including HTTP-controlled configuration, global context annotations, and non-intrusive forward hooks ([PR#19034](https://github.com/sgl-project/sglang/pull/19034), [PR#19068](https://github.com/sgl-project/sglang/pull/19068)).
*   **Robustness:** Now rejects duplicate request IDs to prevent internal state corruption in `DetokenizerManager` ([PR#19035](https://github.com/sgl-project/sglang/pull/19035)).
