# AI Analysis of Pull Requests for vllm-project/vllm during 20260222-20260301

**Key Takeaways:** This weekâ€™s development is dominated by the structural migration toward (1) Model Runner V2 infrastructure, alongside significant progress in (2) ROCm/AITER performance parity, (3) disaggregated serving via KV Connectors, and (4) deep architectural support for the Qwen3.5 and DeepSeek model families across 456 pull requests.

### 1. Core Architecture & Model Runner V2
*   **ModelState Integration**: Progress continues on the `ModelState` abstraction to decouple model execution state from the runner, including adding `ModelStateInterface` [PR#35621](https://github.com/vllm-project/vllm/pull/35621) and migrating MM encoders [PR#35564](https://github.com/vllm-project/vllm/pull/35564).
*   **V2 Functional Parity**: Implementation of Sequence Parallelism (Piecewise CUDA Graph, PP=1) for V2 [PR#35206](https://github.com/vllm-project/vllm/pull/35206) and support for Data Parallelism/Expert Parallelism in speculative decoding [PR#35248](https://github.com/vllm-project/vllm/pull/35248).
*   **Scheduler Robustness**: Fixed a critical infinite hang in the V1 scheduler occurring when requests exceed total KV capacity [PR#35570](https://github.com/vllm-project/vllm/pull/35570) or when `num_gpu_blocks_override` is insufficient [PR#35542](https://github.com/vllm-project/vllm/pull/35542).
*   **UniProcExecutor**: Surfaced exceptions from non-blocking `execute_model` calls to prevent distributed deadlocks [PR#35194](https://github.com/vllm-project/vllm/pull/35194).

### 2. Performance & Kernel Optimizations
*   **Dynamic Attention Routing**: DeepSeek-V3 now utilizes `MLAPathSelector` to dynamically route prefills < 1024 tokens to MHA, yielding a reported ~3x TTFT speedup [PR#35474](https://github.com/vllm-project/vllm/pull/35474).
*   **Fused Triton Kernels**: 
    *   Added a fused concat+quantize kernel for FP8 decode queries in MLA [PR#35095](https://github.com/vllm-project/vllm/pull/35095).
    *   Introduced `fused_evs_mrope` and `fused_pos_embed_interp` for Qwen3-VL preprocessing [PR#35058](https://github.com/vllm-project/vllm/pull/35058).
*   **Mamba/SSM Optimizations**: Significant prefill throughput improvements for Mamba2 SSD via expanded autotune search spaces and hardware-native `exp2` utilization [PR#35397](https://github.com/vllm-project/vllm/pull/35397).
*   **Sampling**: Optimized `sampled_token_ids` using NumPy to reduce `tolist()` overhead, improving E2E throughput by ~0.9% [PR#35446](https://github.com/vllm-project/vllm/pull/35446).

### 3. Hardware Backends
*   **ROCm (AMD)**:
    *   Added AMD AITER MLA fusion (RMSNorm + FP8 quantization) for DeepSeek [PR#35483](https://github.com/vllm-project/vllm/pull/35483).
    *   Improved `wvSplitKrc` kernel by splitting into deterministic and fast paths with a new `--fast-skinny-gemm` flag [PR#35183](https://github.com/vllm-project/vllm/pull/35183).
    *   Derived device capability from GCN arch strings to avoid unnecessary CUDA initialization [PR#35069](https://github.com/vllm-project/vllm/pull/35069).
*   **Blackwell (NVIDIA)**: 
    *   Fixed Marlin W4A8-FP8 capability checks to support SM121 variants [PR#35568](https://github.com/vllm-project/vllm/pull/35568).
    *   Resolved an `AttributeError` in `varlen_fwd` for Blackwell [PR#35341](https://github.com/vllm-project/vllm/pull/35341).
*   **Intel XPU**: Enabled GPUDirect RDMA support for the Nixl connector [PR#35270](https://github.com/vllm-project/vllm/pull/35270).

### 4. KV Connector & Disaggregated Serving
*   **Elastic Expert Parallelism**: Integrated `NIXL-EP` kernels to enable dynamic rank addition/removal without communicator recreation [PR#35627](https://github.com/vllm-project/vllm/pull/35627).
*   **Zero-Overhead Offloading**: Introduced Strategy A (Reuse tracking to skip non-reused block stores) and Strategy B (Adaptive offloading based on TTFT regression monitoring) [PR#35259](https://github.com/vllm-project/vllm/pull/35259).
*   **KV Push**: Prefill nodes can now proactively push KV blocks to decode nodes via Nixl to reduce TTFT [PR#35264](https://github.com/vllm-project/vllm/pull/35264).

### 5. Quantization & Multimodal
*   **MXFP4 Online Support**: Added online MXFP4 quantization support for MoE models, specifically targeting HuggingFace checkpoints at TP=1 [PR#35560](https://github.com/vllm-project/vllm/pull/35560).
*   **MLA Fixes**: Fixed crashes in quantized MLA layers (NVFP4/INT4) by guarding against missing `weight` attributes in `MarlinLinearMethod` [PR#35576](https://github.com/vllm-project/vllm/pull/35576).
*   **Audio/Video**: Nemotron Nano VL now supports audio extraction from MP4 tracks via PyAV [PR#35539](https://github.com/vllm-project/vllm/pull/35539).

### 6. API & Frontend Integration
*   **Responses API**: Added WebSocket transport for OpenAI-compatible Responses API [PR#35492](https://github.com/vllm-project/vllm/pull/35492).
*   **Disaggregated Frontend**: Implemented a lightweight gRPC render server for tokenization and template rendering without requiring a GPU [PR#35493](https://github.com/vllm-project/vllm/pull/35493).
*   **Tool Calling**: Critical fixes for Qwen3Coder streaming parameter loss under speculative decoding [PR#35615](https://github.com/vllm-project/vllm/pull/35615).
